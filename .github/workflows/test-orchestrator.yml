name: Test Orchestrator

on:
  schedule:
    # Run full test suite daily at 2 AM UTC
    - cron: '0 2 * * *'
    # Run quick test suite every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - 'comprehensive'    # All tests
          - 'essential'        # Unit + Integration + E2E (Chromium only)
          - 'performance'      # Performance + Lighthouse only
          - 'accessibility'    # A11y tests only
          - 'quick'           # Unit tests only
          - 'regression'      # E2E + Performance (for releases)
      environment:
        description: 'Environment to test against'
        required: false
        default: 'production'
        type: choice
        options:
          - 'production'
          - 'staging'
          - 'local'
      notify_on_failure:
        description: 'Send notifications on failure'
        required: false
        default: true
        type: boolean

env:
  TEST_SUITE: ${{ inputs.test_suite || 'comprehensive' }}
  TEST_ENVIRONMENT: ${{ inputs.environment || 'production' }}
  NOTIFY_ON_FAILURE: ${{ inputs.notify_on_failure || true }}

permissions:
  contents: read
  checks: write
  pull-requests: write
  actions: write
  issues: write

jobs:
  # Determine which tests to run based on schedule or input
  determine-test-strategy:
    runs-on: ubuntu-latest
    outputs:
      run-unit: ${{ steps.strategy.outputs.run-unit }}
      run-integration: ${{ steps.strategy.outputs.run-integration }}
      run-e2e: ${{ steps.strategy.outputs.run-e2e }}
      run-performance: ${{ steps.strategy.outputs.run-performance }}
      run-a11y: ${{ steps.strategy.outputs.run-a11y }}
      test-environment: ${{ steps.strategy.outputs.test-environment }}
      browser-matrix: ${{ steps.strategy.outputs.browser-matrix }}
      test-description: ${{ steps.strategy.outputs.test-description }}
    steps:
      - name: Determine test strategy
        id: strategy
        run: |
          echo "Determining test strategy for: ${{ env.TEST_SUITE }}"
          
          # Default values
          RUN_UNIT=false
          RUN_INTEGRATION=false
          RUN_E2E=false
          RUN_PERFORMANCE=false
          RUN_A11Y=false
          BROWSER_MATRIX='["chromium"]'
          TEST_ENV="${{ env.TEST_ENVIRONMENT }}"
          
          case "${{ env.TEST_SUITE }}" in
            "comprehensive")
              RUN_UNIT=true
              RUN_INTEGRATION=true
              RUN_E2E=true
              RUN_PERFORMANCE=true
              RUN_A11Y=true
              BROWSER_MATRIX='["chromium", "firefox", "webkit"]'
              TEST_DESCRIPTION="Comprehensive test suite - all tests across all browsers"
              ;;
            "essential")
              RUN_UNIT=true
              RUN_INTEGRATION=true
              RUN_E2E=true
              BROWSER_MATRIX='["chromium"]'
              TEST_DESCRIPTION="Essential test suite - core functionality tests"
              ;;
            "performance")
              RUN_PERFORMANCE=true
              TEST_DESCRIPTION="Performance test suite - Lighthouse and performance metrics"
              ;;
            "accessibility")
              RUN_A11Y=true
              TEST_DESCRIPTION="Accessibility test suite - WCAG compliance validation"
              ;;
            "quick")
              RUN_UNIT=true
              TEST_DESCRIPTION="Quick test suite - unit tests only"
              ;;
            "regression")
              RUN_E2E=true
              RUN_PERFORMANCE=true
              BROWSER_MATRIX='["chromium", "firefox"]'
              TEST_DESCRIPTION="Regression test suite - E2E and performance validation"
              ;;
            *)
              echo "Unknown test suite: ${{ env.TEST_SUITE }}"
              exit 1
              ;;
          esac
          
          # Handle scheduled runs
          if [ "${{ github.event_name }}" = "schedule" ]; then
            HOUR=$(date -u +"%H")
            if [ "$HOUR" = "02" ]; then
              # 2 AM run - comprehensive
              RUN_UNIT=true
              RUN_INTEGRATION=true
              RUN_E2E=true
              RUN_PERFORMANCE=true
              RUN_A11Y=true
              BROWSER_MATRIX='["chromium", "firefox", "webkit"]'
              TEST_DESCRIPTION="Scheduled comprehensive test suite (daily 2 AM)"
            else
              # 6-hour runs - essential only
              RUN_UNIT=true
              RUN_INTEGRATION=true
              RUN_E2E=true
              BROWSER_MATRIX='["chromium"]'
              TEST_DESCRIPTION="Scheduled essential test suite (every 6 hours)"
            fi
            TEST_ENV="production"
          fi
          
          echo "run-unit=$RUN_UNIT" >> $GITHUB_OUTPUT
          echo "run-integration=$RUN_INTEGRATION" >> $GITHUB_OUTPUT
          echo "run-e2e=$RUN_E2E" >> $GITHUB_OUTPUT
          echo "run-performance=$RUN_PERFORMANCE" >> $GITHUB_OUTPUT
          echo "run-a11y=$RUN_A11Y" >> $GITHUB_OUTPUT
          echo "test-environment=$TEST_ENV" >> $GITHUB_OUTPUT
          echo "browser-matrix=$BROWSER_MATRIX" >> $GITHUB_OUTPUT
          echo "test-description=$TEST_DESCRIPTION" >> $GITHUB_OUTPUT
          
          echo "Test Strategy:"
          echo "- Unit: $RUN_UNIT"
          echo "- Integration: $RUN_INTEGRATION"
          echo "- E2E: $RUN_E2E"
          echo "- Performance: $RUN_PERFORMANCE"
          echo "- A11y: $RUN_A11Y"
          echo "- Environment: $TEST_ENV"
          echo "- Browsers: $BROWSER_MATRIX"
          echo "- Description: $TEST_DESCRIPTION"

  # Unit Tests
  unit-tests:
    needs: determine-test-strategy
    if: needs.determine-test-strategy.outputs.run-unit == 'true'
    uses: ./.github/workflows/test-unit.yml
    with:
      node-version: '20'
      coverage-threshold: 80
    secrets: inherit

  # Integration Tests
  integration-tests:
    needs: determine-test-strategy
    if: needs.determine-test-strategy.outputs.run-integration == 'true'
    uses: ./.github/workflows/test-integration.yml
    with:
      node-version: '20'
      environment: ${{ needs.determine-test-strategy.outputs.test-environment }}
    secrets: inherit

  # E2E Tests
  e2e-tests:
    needs: determine-test-strategy
    if: needs.determine-test-strategy.outputs.run-e2e == 'true'
    strategy:
      matrix:
        browser: ${{ fromJSON(needs.determine-test-strategy.outputs.browser-matrix) }}
      fail-fast: false
    uses: ./.github/workflows/test-e2e.yml
    with:
      browser: ${{ matrix.browser }}
      environment: ${{ needs.determine-test-strategy.outputs.test-environment == 'local' && 'http://localhost:4173' || format('https://{0}.rrishmusic.com', needs.determine-test-strategy.outputs.test-environment == 'staging' && 'staging' || 'www') }}
    secrets: inherit

  # Performance Tests
  performance-tests:
    needs: determine-test-strategy
    if: needs.determine-test-strategy.outputs.run-performance == 'true'
    uses: ./.github/workflows/test-performance.yml
    with:
      environment: ${{ needs.determine-test-strategy.outputs.test-environment }}
      performance-budget: 85
      run-lighthouse: true
    secrets: inherit

  # Accessibility Tests
  accessibility-tests:
    needs: determine-test-strategy
    if: needs.determine-test-strategy.outputs.run-a11y == 'true'
    uses: ./.github/workflows/test-a11y.yml
    with:
      compliance-level: 'AA'
      test-environment: ${{ needs.determine-test-strategy.outputs.test-environment }}
    secrets: inherit

  # Collect and report results
  test-orchestrator-summary:
    runs-on: ubuntu-latest
    needs: [determine-test-strategy, unit-tests, integration-tests, e2e-tests, performance-tests, accessibility-tests]
    if: always()
    steps:
      - name: Collect test results
        run: |
          echo "# Test Orchestrator Summary" > orchestrator-summary.md
          echo "" >> orchestrator-summary.md
          echo "**Test Suite**: ${{ env.TEST_SUITE }}" >> orchestrator-summary.md
          echo "**Description**: ${{ needs.determine-test-strategy.outputs.test-description }}" >> orchestrator-summary.md
          echo "**Environment**: ${{ needs.determine-test-strategy.outputs.test-environment }}" >> orchestrator-summary.md
          echo "**Trigger**: ${{ github.event_name }}" >> orchestrator-summary.md
          echo "**Timestamp**: $(date -u)" >> orchestrator-summary.md
          echo "" >> orchestrator-summary.md
          
          # Collect results
          TOTAL_JOBS=0
          FAILED_JOBS=0
          
          echo "## Test Results" >> orchestrator-summary.md
          echo "" >> orchestrator-summary.md
          
          # Unit Tests
          if [ "${{ needs.determine-test-strategy.outputs.run-unit }}" = "true" ]; then
            TOTAL_JOBS=$((TOTAL_JOBS + 1))
            UNIT_STATUS="${{ needs.unit-tests.result }}"
            if [ "$UNIT_STATUS" = "success" ]; then
              echo "- ✅ **Unit Tests**: Passed" >> orchestrator-summary.md
            else
              echo "- ❌ **Unit Tests**: Failed ($UNIT_STATUS)" >> orchestrator-summary.md
              FAILED_JOBS=$((FAILED_JOBS + 1))
            fi
          fi
          
          # Integration Tests
          if [ "${{ needs.determine-test-strategy.outputs.run-integration }}" = "true" ]; then
            TOTAL_JOBS=$((TOTAL_JOBS + 1))
            INTEGRATION_STATUS="${{ needs.integration-tests.result }}"
            if [ "$INTEGRATION_STATUS" = "success" ]; then
              echo "- ✅ **Integration Tests**: Passed" >> orchestrator-summary.md
            else
              echo "- ❌ **Integration Tests**: Failed ($INTEGRATION_STATUS)" >> orchestrator-summary.md
              FAILED_JOBS=$((FAILED_JOBS + 1))
            fi
          fi
          
          # E2E Tests
          if [ "${{ needs.determine-test-strategy.outputs.run-e2e }}" = "true" ]; then
            TOTAL_JOBS=$((TOTAL_JOBS + 1))
            E2E_STATUS="${{ needs.e2e-tests.result }}"
            if [ "$E2E_STATUS" = "success" ]; then
              echo "- ✅ **E2E Tests**: Passed" >> orchestrator-summary.md
            else
              echo "- ❌ **E2E Tests**: Failed ($E2E_STATUS)" >> orchestrator-summary.md
              FAILED_JOBS=$((FAILED_JOBS + 1))
            fi
          fi
          
          # Performance Tests
          if [ "${{ needs.determine-test-strategy.outputs.run-performance }}" = "true" ]; then
            TOTAL_JOBS=$((TOTAL_JOBS + 1))
            PERF_STATUS="${{ needs.performance-tests.result }}"
            if [ "$PERF_STATUS" = "success" ]; then
              echo "- ✅ **Performance Tests**: Passed" >> orchestrator-summary.md
            else
              echo "- ❌ **Performance Tests**: Failed ($PERF_STATUS)" >> orchestrator-summary.md
              FAILED_JOBS=$((FAILED_JOBS + 1))
            fi
          fi
          
          # Accessibility Tests
          if [ "${{ needs.determine-test-strategy.outputs.run-a11y }}" = "true" ]; then
            TOTAL_JOBS=$((TOTAL_JOBS + 1))
            A11Y_STATUS="${{ needs.accessibility-tests.result }}"
            if [ "$A11Y_STATUS" = "success" ]; then
              echo "- ✅ **Accessibility Tests**: Passed" >> orchestrator-summary.md
            else
              echo "- ❌ **Accessibility Tests**: Failed ($A11Y_STATUS)" >> orchestrator-summary.md
              FAILED_JOBS=$((FAILED_JOBS + 1))
            fi
          fi
          
          echo "" >> orchestrator-summary.md
          echo "## Summary" >> orchestrator-summary.md
          echo "- **Total Test Categories**: $TOTAL_JOBS" >> orchestrator-summary.md
          echo "- **Failed Categories**: $FAILED_JOBS" >> orchestrator-summary.md
          echo "- **Success Rate**: $(( (TOTAL_JOBS - FAILED_JOBS) * 100 / TOTAL_JOBS ))%" >> orchestrator-summary.md
          
          if [ $FAILED_JOBS -gt 0 ]; then
            echo "- **Overall Status**: ❌ FAILED" >> orchestrator-summary.md
            echo "ORCHESTRATOR_FAILED=true" >> $GITHUB_ENV
          else
            echo "- **Overall Status**: ✅ PASSED" >> orchestrator-summary.md
          fi
          
          echo "" >> orchestrator-summary.md
          echo "---" >> orchestrator-summary.md
          echo "*Generated by Test Orchestrator - $(date -u)*" >> orchestrator-summary.md

      - name: Upload orchestrator summary
        uses: actions/upload-artifact@v4
        with:
          name: test-orchestrator-summary
          path: orchestrator-summary.md
          retention-days: 30

      - name: Create GitHub issue on failure
        if: env.ORCHESTRATOR_FAILED == 'true' && env.NOTIFY_ON_FAILURE == 'true' && github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let issueBody = '## 🚨 Scheduled Test Suite Failure\n\n';
            
            if (fs.existsSync('orchestrator-summary.md')) {
              const summary = fs.readFileSync('orchestrator-summary.md', 'utf8');
              issueBody += summary;
            } else {
              issueBody += 'Test orchestrator failed but summary is not available.\n';
            }
            
            issueBody += '\n\n## Next Steps\n';
            issueBody += '1. Review the failed test workflows\n';
            issueBody += '2. Check for any infrastructure or dependency issues\n';
            issueBody += '3. Fix any identified problems\n';
            issueBody += '4. Re-run the test suite to verify fixes\n';
            issueBody += '5. Close this issue once all tests pass\n\n';
            issueBody += `**Workflow Run**: ${context.payload.repository.html_url}/actions/runs/${context.runId}\n`;
            
            // Check if there's already an open issue for test failures
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'automated-test-failure'
            });
            
            if (issues.length === 0) {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `🚨 Scheduled Test Suite Failure - ${new Date().toISOString().split('T')[0]}`,
                body: issueBody,
                labels: ['automated-test-failure', 'bug', 'priority-high']
              });
            } else {
              // Update existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues[0].number,
                body: `## 🔄 Additional Test Failure\n\n${issueBody}`
              });
            }

      - name: Send Slack notification on failure
        if: env.ORCHESTRATOR_FAILED == 'true' && env.NOTIFY_ON_FAILURE == 'true'
        run: |
          # This would integrate with your Slack webhook if configured
          echo "Would send Slack notification about test failure"
          echo "Test suite '${{ env.TEST_SUITE }}' failed in environment '${{ needs.determine-test-strategy.outputs.test-environment }}'"
          
          # Example webhook call (requires SLACK_WEBHOOK_URL secret):
          # curl -X POST -H 'Content-type: application/json' \
          #   --data '{"text":"🚨 Test Suite Failure: ${{ env.TEST_SUITE }} failed in ${{ needs.determine-test-strategy.outputs.test-environment }}"}' \
          #   ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Fail workflow if tests failed
        if: env.ORCHESTRATOR_FAILED == 'true'
        run: |
          echo "❌ Test Orchestrator failed - one or more test categories failed"
          exit 1

  # Clean up old test artifacts (runs only on schedule)
  cleanup-artifacts:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
      - name: Cleanup old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const cutoffDate = new Date();
            cutoffDate.setDate(cutoffDate.getDate() - 30); // Keep artifacts for 30 days
            
            const { data: artifacts } = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100
            });
            
            let deletedCount = 0;
            for (const artifact of artifacts.artifacts) {
              const artifactDate = new Date(artifact.created_at);
              if (artifactDate < cutoffDate) {
                try {
                  await github.rest.actions.deleteArtifact({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    artifact_id: artifact.id
                  });
                  deletedCount++;
                  console.log(`Deleted artifact: ${artifact.name} (${artifact.created_at})`);
                } catch (error) {
                  console.log(`Failed to delete artifact ${artifact.name}: ${error.message}`);
                }
              }
            }
            
            console.log(`Cleaned up ${deletedCount} old artifacts`);

      - name: Cleanup old workflow runs
        uses: actions/github-script@v7
        with:
          script: |
            const cutoffDate = new Date();
            cutoffDate.setDate(cutoffDate.getDate() - 90); // Keep runs for 90 days
            
            const workflows = ['test-unit.yml', 'test-integration.yml', 'test-e2e.yml', 'test-performance.yml', 'test-a11y.yml'];
            
            for (const workflowFile of workflows) {
              try {
                const { data: workflow } = await github.rest.actions.getWorkflowByFilename({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_filename: workflowFile
                });
                
                const { data: runs } = await github.rest.actions.listWorkflowRuns({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: workflow.id,
                  status: 'completed',
                  per_page: 100
                });
                
                let deletedRuns = 0;
                for (const run of runs.workflow_runs) {
                  const runDate = new Date(run.created_at);
                  if (runDate < cutoffDate) {
                    try {
                      await github.rest.actions.deleteWorkflowRun({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        run_id: run.id
                      });
                      deletedRuns++;
                    } catch (error) {
                      console.log(`Failed to delete run ${run.id}: ${error.message}`);
                    }
                  }
                }
                
                console.log(`Cleaned up ${deletedRuns} old runs for ${workflowFile}`);
              } catch (error) {
                console.log(`Error processing workflow ${workflowFile}: ${error.message}`);
              }
            }